# Flappy Bird - Reinforcement Learning (DQN)

Flappy Bird oyununu Deep Q-Network (DQN) algoritmasÄ± ile kendi kendine Ã¶ÄŸrenen bir yapay zeka projesi.

## Proje YapÄ±sÄ±

```
flappybird/
â”œâ”€â”€ game.py          # Flappy Bird oyun mantÄ±ÄŸÄ±
â”œâ”€â”€ agent.py         # DQN Agent implementasyonu
â”œâ”€â”€ train.py         # EÄŸitim scripti
â”œâ”€â”€ play.py          # EÄŸitilmiÅŸ modeli izleme
â”œâ”€â”€ requirements.txt # Gerekli kÃ¼tÃ¼phaneler
â”œâ”€â”€ models/          # Kaydedilen modeller
â””â”€â”€ README.md        # Bu dosya
```

## Kurulum

### 1. Gerekli KÃ¼tÃ¼phaneleri YÃ¼kle

```bash
pip install -r requirements.txt
```

YÃ¼klenen kÃ¼tÃ¼phaneler:
- **pygame**: Oyun gÃ¶rselleÅŸtirmesi
- **torch**: Deep Learning (DQN)
- **numpy**: Matematiksel iÅŸlemler
- **matplotlib**: Grafik gÃ¶rselleÅŸtirme

## KullanÄ±m

### 1. Oyunu Test Et (Manuel)

Ã–nce oyunun Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin:

```bash
python game.py
```

Rastgele action'larla oyun oynanÄ±r. Pencereyi kapatÄ±n.

### 2. Agent'Ä± EÄŸit

#### SÄ±fÄ±rdan EÄŸitim

DQN agent'Ä±nÄ± sÄ±fÄ±rdan eÄŸitmek iÃ§in:

```bash
python train.py
```

veya parametrelerle:

```bash
python train.py --episodes 500 --render
```

#### Checkpoint'tan Devam Etme

EÄŸitilmiÅŸ bir modelin Ã¼zerine devam etmek iÃ§in:

```bash
python train.py --continue
```

veya daha fazla parametreyle:

```bash
python train.py --continue --episodes 500 --no-render
```

**Komut SatÄ±rÄ± Parametreleri:**
- `--episodes`: EÄŸitilecek episode sayÄ±sÄ± (varsayÄ±lan: 2000)
- `--render`: GÃ¶rselleÅŸtirme aÃ§Ä±k (yavaÅŸ ama izlenebilir)
- `--continue`: Mevcut checkpoint'tan devam et
- `--save-interval`: Model kayÄ±t aralÄ±ÄŸÄ± (varsayÄ±lan: 50)
- `--model-path`: Model kayÄ±t yolu (varsayÄ±lan: models/flappy_dqn.pth)

**EÄŸitim SÄ±rasÄ±nda:**
- Her episode'un skoru, ortalama skor ve loss deÄŸerleri gÃ¶sterilir
- Her 50 episode'da **3 farklÄ± model** kaydedilir:
  - **Latest** (`flappy_dqn.pth`): En son checkpoint
  - **Best** (`flappy_dqn_best.pth`): En yÃ¼ksek model skoru
  - **Stable** (`flappy_dqn_stable.pth`): En tutarlÄ± performans
- Training history `models/training_history.json` dosyasÄ±na kaydedilir
- EÄŸitim bitince `training_results.png` grafiÄŸi oluÅŸturulur

**Model DeÄŸerlendirme Kriteri:**
```
Model Score = (avg_score Ã— 0.4) + (max_score Ã— 0.2) +
              (consistency_bonus Ã— 0.2) + (avg_frames/100 Ã— 0.2)
```
- Ortalama skor: %40 aÄŸÄ±rlÄ±k
- En yÃ¼ksek skor: %20 aÄŸÄ±rlÄ±k
- Consistency (dÃ¼ÅŸÃ¼k std dev): %20 aÄŸÄ±rlÄ±k
- Hayatta kalma sÃ¼resi: %20 aÄŸÄ±rlÄ±k

**Not:** Ä°lk 50-100 episode'da agent Ã§ok kÃ¶tÃ¼ olacak (exploration fazÄ±). Zamanla Ã¶ÄŸrenmeye baÅŸlayacak!

**Ã–rnek Senaryo:**
```bash
# 1. Ä°lk eÄŸitim: 2000 episode (hÄ±zlÄ±, gÃ¶rselleÅŸtirme kapalÄ±)
python train.py

# 2. GÃ¶rselleÅŸtirme ile izlemek istersen
python train.py --render --episodes 500

# 3. Mevcut modelden devam et (2000 episode daha)
python train.py --continue

# 4. KÄ±sa test eÄŸitimi (10 episode)
python train.py --episodes 10 --render
```

### 3. EÄŸitilmiÅŸ Agent'Ä± Ä°zle

EÄŸitim tamamlandÄ±ktan sonra:

```bash
python play.py
```

5 oyun oynanÄ±r ve performans gÃ¶sterilir.

### 4. Ä°nteraktif Mod

Hem manuel hem otomatik oynayabilirsiniz:

```bash
python play.py interactive
```

**Kontroller:**
- **SPACE**: Manuel zÄ±pla
- **A**: Otomatik/Manuel mod deÄŸiÅŸtir
- **Q**: Ã‡Ä±kÄ±ÅŸ

## NasÄ±l Ã‡alÄ±ÅŸÄ±r?

### DQN (Deep Q-Network)

Agent, **Reinforcement Learning** ile kendi kendine Ã¶ÄŸrenir:

1. **State (Durum)**: Agent 4 deÄŸer gÃ¶rÃ¼r
   - KuÅŸun Y pozisyonu
   - KuÅŸun hÄ±zÄ± (velocity)
   - En yakÄ±n borunun X uzaklÄ±ÄŸÄ±
   - BoÅŸluÄŸun Y pozisyonu

2. **Action (Hareket)**: 2 seÃ§enek
   - `0`: HiÃ§bir ÅŸey yapma (dÃ¼ÅŸ)
   - `1`: ZÄ±pla

3. **Reward (Ã–dÃ¼l) - GeliÅŸmiÅŸ Reward Shaping**:
   - Hayatta kalma: +0.1 (her frame)
   - BoÅŸluÄŸun ortasÄ±na yakÄ±nlÄ±k: +0.0 ~ +0.5 (yakÄ±nsa daha fazla)
   - Stabil uÃ§uÅŸ bonusu: +0.05 (hÄ±z dengeli ise)
   - Borudan geÃ§me: +10 (ana Ã¶dÃ¼l)
   - Ã‡arpma/Ã–lme: -10

4. **Learning (Ã–ÄŸrenme)**:
   - Neural Network (4 â†’ 64 â†’ 64 â†’ 2)
   - Experience Replay Buffer
   - Epsilon-greedy exploration

### Ã–ÄŸrenme SÃ¼reci

GeliÅŸmiÅŸ reward shaping sayesinde daha hÄ±zlÄ± Ã¶ÄŸrenir:

```
Episode 1-50    : Rastgele exploration, reward shaping etkisi baÅŸlar
Episode 50-200  : BoÅŸluÄŸun ortasÄ±nda kalmayÄ± Ã¶ÄŸrenir
Episode 200-500 : TutarlÄ± bir ÅŸekilde borulardan geÃ§er
Episode 500+    : YÃ¼ksek skorlar (20-50+), Ã§ok stabil performans
```

Beklenen sonuÃ§lar:
- **Episode 100**: Ortalama skor ~2-5
- **Episode 500**: Ortalama skor ~10-20
- **Episode 1000**: Ortalama skor ~20-40
- **Episode 2000**: Ortalama skor ~30-60+

## Hyperparameters

[agent.py](agent.py) iÃ§inde ayarlanabilir:

```python
learning_rate = 0.001     # Ã–ÄŸrenme hÄ±zÄ±
gamma = 0.95              # Gelecek Ã¶dÃ¼llerin deÄŸeri
epsilon = 1.0 â†’ 0.01      # Exploration oranÄ±
batch_size = 32           # EÄŸitim batch boyutu
buffer_size = 10000       # Replay buffer
```

## Hangi Modeli KullanmalÄ±?

EÄŸitim sonunda 3 model elde edersiniz:

1. **flappy_dqn_best.pth**
   - En yÃ¼ksek genel performans
   - YÃ¼ksek skor + tutarlÄ±lÄ±k dengesi
   - **Ã–nerilen**: Genel kullanÄ±m iÃ§in

2. **flappy_dqn_stable.pth**
   - En tutarlÄ± performans (dÃ¼ÅŸÃ¼k std dev)
   - Her oyunda benzer sonuÃ§lar
   - **Ã–nerilen**: YarÄ±ÅŸ/demo iÃ§in

3. **flappy_dqn.pth** (latest)
   - En son checkpoint
   - Devam etmek iÃ§in kullanÄ±lÄ±r

**Test iÃ§in:**
```bash
# Best model ile oyna
python play.py --model-path models/flappy_dqn_best.pth

# Stable model ile oyna
python play.py --model-path models/flappy_dqn_stable.pth
```

## Grafik SonuÃ§lar

EÄŸitim bitince `training_results.png` oluÅŸturulur:
- SkorlarÄ±n zamanla artÄ±ÅŸÄ±
- Loss deÄŸiÅŸimi
- Epsilon (exploration) azalmasÄ±
- Skor daÄŸÄ±lÄ±mÄ±

## Sorun Giderme

**Agent hiÃ§ Ã¶ÄŸrenmiyor:**
- Daha fazla episode eÄŸitin (2000+)
- Reward shaping zaten ekli, sabÄ±rlÄ± olun
- Ä°lk 100-200 episode kÃ¶tÃ¼ olmasÄ± normal

**EÄŸitim Ã§ok yavaÅŸ:**
- VarsayÄ±lan zaten `render=False` (hÄ±zlÄ±)
- 2000 episode ~30-60 dakika (CPU'ya gÃ¶re)

**Model yÃ¼klenmiyor:**
- Ã–nce `train.py` ile modeli eÄŸitin
- `models/` klasÃ¶rÃ¼nde 3 model olmalÄ±:
  - `flappy_dqn.pth`
  - `flappy_dqn_best.pth`
  - `flappy_dqn_stable.pth`

**Best/Stable model gÃ¼ncellenmiyor:**
- Ä°lk 100 episode'da normal
- Model yeterince iyi olmayabilir, daha fazla eÄŸitin

## Lisans

EÄŸitim amaÃ§lÄ± bir projedir. Ã–zgÃ¼rce kullanabilirsiniz.

---

**Ä°yi eÄŸitimler! ğŸš€**
