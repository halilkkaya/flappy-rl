# Flappy Bird - Reinforcement Learning (DQN)

Flappy Bird oyununu Deep Q-Network (DQN) algoritmasÄ± ile kendi kendine Ã¶ÄŸrenen bir yapay zeka projesi.

## Proje YapÄ±sÄ±

```
flappybird/
â”œâ”€â”€ game.py          # Flappy Bird oyun mantÄ±ÄŸÄ±
â”œâ”€â”€ agent.py         # DQN Agent implementasyonu
â”œâ”€â”€ train.py         # EÄŸitim scripti
â”œâ”€â”€ play.py          # EÄŸitilmiÅŸ modeli izleme
â”œâ”€â”€ requirements.txt # Gerekli kÃ¼tÃ¼phaneler
â”œâ”€â”€ models/          # Kaydedilen modeller
â””â”€â”€ README.md        # Bu dosya
```

## Kurulum

### 1. Gerekli KÃ¼tÃ¼phaneleri YÃ¼kle

```bash
pip install -r requirements.txt
```

YÃ¼klenen kÃ¼tÃ¼phaneler:
- **pygame**: Oyun gÃ¶rselleÅŸtirmesi
- **torch**: Deep Learning (DQN)
- **numpy**: Matematiksel iÅŸlemler
- **matplotlib**: Grafik gÃ¶rselleÅŸtirme

## KullanÄ±m

### 1. Oyunu Test Et (Manuel)

Ã–nce oyunun Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin:

```bash
python game.py
```

Rastgele action'larla oyun oynanÄ±r. Pencereyi kapatÄ±n.

### 2. Agent'Ä± EÄŸit

#### SÄ±fÄ±rdan EÄŸitim

DQN agent'Ä±nÄ± sÄ±fÄ±rdan eÄŸitmek iÃ§in:

```bash
python train.py
```

veya parametrelerle:

```bash
python train.py --episodes 500 --render
```

#### Checkpoint'tan Devam Etme

EÄŸitilmiÅŸ bir modelin Ã¼zerine devam etmek iÃ§in:

```bash
python train.py --continue
```

veya daha fazla parametreyle:

```bash
python train.py --continue --episodes 500 --no-render
```

**Komut SatÄ±rÄ± Parametreleri:**
- `--episodes`: EÄŸitilecek episode sayÄ±sÄ± (varsayÄ±lan: 500)
- `--render`: GÃ¶rselleÅŸtirme aÃ§Ä±k (varsayÄ±lan)
- `--no-render`: GÃ¶rselleÅŸtirme kapalÄ± (Ã§ok hÄ±zlÄ± eÄŸitim!)
- `--continue`: Mevcut checkpoint'tan devam et
- `--save-interval`: Model kayÄ±t aralÄ±ÄŸÄ± (varsayÄ±lan: 50)
- `--model-path`: Model kayÄ±t yolu (varsayÄ±lan: models/flappy_dqn.pth)

**EÄŸitim SÄ±rasÄ±nda:**
- Her episode'un skoru, ortalama skor ve loss deÄŸerleri gÃ¶sterilir
- Model ve training history her 50 episode'da kaydedilir
  - Model: `models/flappy_dqn.pth`
  - History: `models/training_history.json`
- EÄŸitim bitince `training_results.png` grafiÄŸi oluÅŸturulur (tÃ¼m episode'larÄ± gÃ¶sterir)

**Not:** Ä°lk 50-100 episode'da agent Ã§ok kÃ¶tÃ¼ olacak (exploration fazÄ±). Zamanla Ã¶ÄŸrenmeye baÅŸlayacak!

**Ã–rnek Senaryo:**
```bash
# 1. Ä°lk eÄŸitim: 500 episode
python train.py --episodes 500 --render

# 2. Model iyi ama daha da iyileÅŸebilir, 500 episode daha ekle
python train.py --continue --episodes 500 --render

# 3. HÄ±zlÄ± fine-tuning: 1000 episode daha, gÃ¶rselleÅŸtirme kapalÄ±
python train.py --continue --episodes 1000 --no-render
```

### 3. EÄŸitilmiÅŸ Agent'Ä± Ä°zle

EÄŸitim tamamlandÄ±ktan sonra:

```bash
python play.py
```

5 oyun oynanÄ±r ve performans gÃ¶sterilir.

### 4. Ä°nteraktif Mod

Hem manuel hem otomatik oynayabilirsiniz:

```bash
python play.py interactive
```

**Kontroller:**
- **SPACE**: Manuel zÄ±pla
- **A**: Otomatik/Manuel mod deÄŸiÅŸtir
- **Q**: Ã‡Ä±kÄ±ÅŸ

## NasÄ±l Ã‡alÄ±ÅŸÄ±r?

### DQN (Deep Q-Network)

Agent, **Reinforcement Learning** ile kendi kendine Ã¶ÄŸrenir:

1. **State (Durum)**: Agent 4 deÄŸer gÃ¶rÃ¼r
   - KuÅŸun Y pozisyonu
   - KuÅŸun hÄ±zÄ± (velocity)
   - En yakÄ±n borunun X uzaklÄ±ÄŸÄ±
   - BoÅŸluÄŸun Y pozisyonu

2. **Action (Hareket)**: 2 seÃ§enek
   - `0`: HiÃ§bir ÅŸey yapma (dÃ¼ÅŸ)
   - `1`: ZÄ±pla

3. **Reward (Ã–dÃ¼l)**:
   - Hayatta kalma: +0.1 (her frame)
   - Borudan geÃ§me: +10
   - Ã‡arpma/Ã–lme: -10

4. **Learning (Ã–ÄŸrenme)**:
   - Neural Network (4 â†’ 64 â†’ 64 â†’ 2)
   - Experience Replay Buffer
   - Epsilon-greedy exploration

### Ã–ÄŸrenme SÃ¼reci

```
Episode 1-100   : Agent rastgele hareket eder (exploration)
Episode 100-300 : Agent Ã¶ÄŸrenmeye baÅŸlar, bazen baÅŸarÄ±lÄ± olur
Episode 300+    : Agent iyi oynar, yÃ¼ksek skorlar alÄ±r
```

## Hyperparameters

[agent.py](agent.py) iÃ§inde ayarlanabilir:

```python
learning_rate = 0.001     # Ã–ÄŸrenme hÄ±zÄ±
gamma = 0.95              # Gelecek Ã¶dÃ¼llerin deÄŸeri
epsilon = 1.0 â†’ 0.01      # Exploration oranÄ±
batch_size = 32           # EÄŸitim batch boyutu
buffer_size = 10000       # Replay buffer
```

## Ä°yileÅŸtirme Ã–nerileri

Daha iyi sonuÃ§lar iÃ§in:

1. **Daha fazla episode** eÄŸit (1000-2000)
2. **Render=False** yaparak hÄ±zlÄ± eÄŸit
3. **Learning rate** ve **gamma** deÄŸerlerini optimize et
4. **Double DQN** veya **Dueling DQN** kullan
5. Daha fazla **hidden layer** ekle

## Grafik SonuÃ§lar

EÄŸitim bitince `training_results.png` oluÅŸturulur:
- SkorlarÄ±n zamanla artÄ±ÅŸÄ±
- Loss deÄŸiÅŸimi
- Epsilon (exploration) azalmasÄ±
- Skor daÄŸÄ±lÄ±mÄ±

## Sorun Giderme

**Agent hiÃ§ Ã¶ÄŸrenmiyor:**
- Daha fazla episode eÄŸitin (500+)
- Learning rate'i artÄ±rÄ±n (0.001 â†’ 0.005)
- Reward sistemini ayarlayÄ±n

**Oyun Ã§ok yavaÅŸ:**
- `train.py` iÃ§inde `render=False` yapÄ±n
- FPS sÄ±nÄ±rÄ±nÄ± kaldÄ±rÄ±n

**Model yÃ¼klenmiyor:**
- Ã–nce `train.py` ile modeli eÄŸitin
- `models/flappy_dqn.pth` dosyasÄ±nÄ±n var olduÄŸunu kontrol edin

## Lisans

EÄŸitim amaÃ§lÄ± bir projedir. Ã–zgÃ¼rce kullanabilirsiniz.

---

**Ä°yi eÄŸitimler! ğŸš€**
